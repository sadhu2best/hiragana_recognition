{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-88ea3969959b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#from loadData import loadDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#from loadModel import loadModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from loadData import loadDataset\n",
    "#from loadModel import loadModel\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from frontend import createGUI\n",
    "import os\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import sys, os, re\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageOps\n",
    "from PIL import Image\n",
    "\n",
    "PATH = os.getcwd()\n",
    "#loads dataset from path, gets file name and converts to bit array\n",
    "def loadDataset(dataset, loadAgain = False):\n",
    "    if(loadAgain):\n",
    "        dataPath = \"%s/../dataSets/%s\"%(PATH,dataset)\n",
    "        characters = os.listdir(dataPath)\n",
    "        characters = [char for char in characters if os.path.isdir(\"%s/%s\"%(dataPath, char))]\n",
    "        im = Image.open(\"%s/%s/%s\"%(dataPath, characters[0], os.listdir(\"%s/%s\"%(dataPath, characters[0]))[0] ))\n",
    "        width, height = im.size\n",
    "        del im\n",
    "        X = np.zeros((1,height, width))\n",
    "        Y = list()\n",
    "        paths = list()\n",
    "        counter = 0\n",
    "        perc = 0\n",
    "        for char in characters:\n",
    "            images = os.listdir(\"%s/%s\"%(dataPath,char))\n",
    "            for im in images:\n",
    "                counter += 1\n",
    "                if(counter%800 == 0):\n",
    "                    perc += 1\n",
    "                    print(str(perc)+\"%\")\n",
    "                imgPath = \"%s/%s/%s\"%(dataPath, char, im)\n",
    "                paths.append(imgPath)\n",
    "                Y.append(char)\n",
    "                img = image.load_img(imgPath)\n",
    "                img = img.convert('L')\n",
    "                x = image.img_to_array(img)\n",
    "                img.close()\n",
    "                X = np.concatenate((X, x))\n",
    "        X = np.delete(X, 0, axis=0)\n",
    "        Y = np.asarray(Y)\n",
    "        paths = np.asarray(paths)\n",
    "        numpyPath = \"%s/../numpyData/%s\"%(PATH, dataset)\n",
    "        XFile = open(\"%s/X.npy\"%(numpyPath), 'bw')\n",
    "        YFile = open(\"%s/Y.npy\"%(numpyPath), 'bw')\n",
    "        PathFile = open(\"%s/Path.npy\"%(numpyPath), 'bw')\n",
    "        np.save(XFile, X)\n",
    "        np.save(YFile, Y)\n",
    "        np.save(PathFile, paths)\n",
    "        XFile.close()\n",
    "        YFile.close()\n",
    "        PathFile.close()\n",
    "        return (X, Y, paths)\n",
    "    else:\n",
    "        numpyPath = \"%s/../numpyData/%s\"%(PATH, dataset)\n",
    "        XFile = open(\"%s/X.npy\"%numpyPath, 'br')\n",
    "        YFile = open(\"%s/Y.npy\"%numpyPath, 'br')\n",
    "        PathFile = open(\"%s/Path.npy\"%numpyPath, 'br')\n",
    "        X = np.load(XFile)\n",
    "        Y = np.load(YFile)\n",
    "        paths = np.load(PathFile)\n",
    "        XFile.close()\n",
    "        YFile.close()\n",
    "        PathFile.close()\n",
    "        return (X,Y,paths)\n",
    "\n",
    "    from keras.models import model_from_json\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "\n",
    "def loadModel():\n",
    "    json_file = open(PATH+'/../models/model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(PATH+\"/../models/model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return loaded_model\n",
    "    \n",
    "#DON'T TOUCH, I DON'T KNOW WHAT IT DOES##\n",
    "from keras import backend as K         ##\n",
    "K.set_image_dim_ordering('th')         ##\n",
    "#########################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from subSetMaker import subSetMaker\n",
    "\n",
    "PATH = os.getcwd()\n",
    "\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(50, (5, 5), input_shape=(1, 84, 83), data_format='channels_first', activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# DATASET = \"Hiragana73\"\n",
    "DATASET = \"HiraganaGit\"\n",
    "\n",
    "#SWITCH THE LINES BELOW IF YOU NEED TO LOAD ALL THE DATA FROM THE DATASET AGAIN\n",
    "X, Y, imgPaths = loadDataset(DATASET, loadAgain=True)\n",
    "#X, Y, imgPaths = loadDataset(DATASET, loadAgain=False)\n",
    "\n",
    "X /= 255\n",
    "\n",
    "#X has format (height, width, N)\n",
    "\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.seed(3)\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "Y = Y[indices]\n",
    "imgPaths = imgPaths[indices]\n",
    "\n",
    "N = X.shape[0]\n",
    "\n",
    "TRAIN_RATIO = 80\n",
    "\n",
    "Ntrain = int(N*TRAIN_RATIO/100)\n",
    "Ntest = int(N*(100-TRAIN_RATIO)/100)\n",
    "\n",
    "x_train = X[:Ntrain].reshape((Ntrain, 1, 84, 83))\n",
    "y_train = Y[:Ntrain]\n",
    "paths_train = imgPaths[:Ntrain]\n",
    "\n",
    "x_test = X[Ntrain:Ntrain+Ntest].reshape((Ntest, 1, 84, 83))\n",
    "y_test = Y[Ntrain:Ntrain+Ntest]\n",
    "paths_test = imgPaths[Ntrain:Ntrain+Ntest]\n",
    "orig_y_test = np.copy(y_test)\n",
    "\n",
    "#CONVERT CLASSES TO NUMBERS\n",
    "\n",
    "labelToNumber = dict()\n",
    "numberToLabel = dict()\n",
    "counter = 0\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i] not in labelToNumber):\n",
    "        labelToNumber[y_train[i]] = counter\n",
    "        numberToLabel[counter] = y_train[i]\n",
    "        counter += 1\n",
    "    y_train[i] = labelToNumber[y_train[i]]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i] not in labelToNumber):\n",
    "        labelToNumber[y_test[i]] = counter\n",
    "        numberToLabel[counter]=y_test[i]\n",
    "        counter += 1\n",
    "    y_test[i] = labelToNumber[y_test[i]]\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "#NUMBER OF CLASSES\n",
    "num_classes = y_train.shape[1]\n",
    "num_epochs = 5\n",
    "\n",
    "trainAgain = False\n",
    "if trainAgain:\n",
    "    model = baseline_model()\n",
    "    metrics = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs=num_epochs, batch_size=10) #returns val_loss, val_acc, loss, acc\n",
    "    modelJSON = model.to_json()\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(PATH+\"/../models/model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    #serialize training history\n",
    "    with open(PATH+\"/../models/history.pkl\", 'wb') as file_pi:\n",
    "        pkl.dump(metrics.history, file_pi)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(PATH+\"/../models/model.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "else:\n",
    "    model = loadModel()\n",
    "    y_pred = model.predict_classes(x_test)\n",
    "    y_pred = [numberToLabel[char] for char in y_pred]\n",
    "    createGUI(orig_y_test, y_pred, paths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
